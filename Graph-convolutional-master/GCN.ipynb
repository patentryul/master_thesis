{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data import generate_batches\n",
    "from data import prepare_data\n",
    "from data import data_to_index\n",
    "from data import DEP_LABELS\n",
    "\n",
    "from model.graph import Sintactic_GCN\n",
    "from model.encoder import Encoder\n",
    "from model.decoder import Decoder_luong\n",
    "\n",
    "from BLEU import BLEU\n",
    "\n",
    "from utils import time_since\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from stanfordcorenlp import StanfordCoreNLP \n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from validation import Evaluator\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = True\n",
    "MAX_LENGTH = 100\n",
    "\n",
    "SPLIT_TRAIN = 0.7\n",
    "SPLIT_VALID = 0.15\n",
    "# The rest is for test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare vocabulary and pairs for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 118964 sentence pairs\n",
      "Filtered to 85785 pairs\n",
      "Creating vocab...\n",
      "Indexed 12436 words in input language, 22765 words in output\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepare_data('en', 'spa', max_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting pairs into test, val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.shuffle(pairs)\n",
    "pairs_train = pairs[:int(len(pairs) * SPLIT_TRAIN)]\n",
    "pairs_valid = pairs[int(len(pairs) * SPLIT_TRAIN):int(len(pairs) * (SPLIT_TRAIN + SPLIT_VALID))]\n",
    "pairs_test = pairs[int(len(pairs) * (SPLIT_TRAIN + SPLIT_VALID)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60049, 12868, 12868)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs_train), len(pairs_valid), len(pairs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the adjacency matrix for the pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = StanfordCoreNLP(r'/home/krivas/stanford-corenlp-full-2018-02-27/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def get_adjacency_matrix(pairs):\n",
    "    arr_dep = []\n",
    "    for pair in tqdm(pairs):\n",
    "        arr_dep.append(nlp.dependency_parse(pair[0]))\n",
    "    return np.array(arr_dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60049/60049 [07:20<00:00, 136.33it/s]\n",
      "100%|██████████| 12868/12868 [02:03<00:00, 104.42it/s]\n",
      "100%|██████████| 12868/12868 [02:29<00:00, 86.21it/s]\n"
     ]
    }
   ],
   "source": [
    "arr_dep_train = get_adjacency_matrix(pairs_train)\n",
    "arr_dep_valid = get_adjacency_matrix(pairs_valid)\n",
    "arr_dep_test = get_adjacency_matrix(pairs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting words to index in pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_train = data_to_index(pairs_train, input_lang, output_lang)\n",
    "pairs_valid = data_to_index(pairs_valid, input_lang, output_lang)\n",
    "pairs_test = data_to_index(pairs_test, input_lang, output_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_batch_luong(batch_size, input_batches, target_batches, train=True, adj_arc_in=None, adj_arc_out=None, adj_lab_in=None, adj_lab_out=None, mask_in=None, mask_out=None, mask_loop=None):\n",
    "        \n",
    "    hidden = encoder.init_hidden(batch_size)\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batches, hidden)\n",
    "    decoder_input = Variable(torch.LongTensor([input_lang.vocab.stoi[\"<sos>\"]] * batch_size))\n",
    "    \n",
    "    if gcn1:\n",
    "        encoder_outputs = gcn1(encoder_outputs,\n",
    "                             adj_arc_in, adj_arc_out,\n",
    "                             adj_lab_in, adj_lab_out,\n",
    "                             mask_in, mask_out,  \n",
    "                             mask_loop)\n",
    "    \n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_context = Variable(torch.zeros(batch_size, decoder.hidden_size)) \n",
    "    \n",
    "    all_decoder_outputs = Variable(torch.zeros(target_batches.data.size()[0], batch_size, len(output_lang.vocab.itos)))\n",
    "\n",
    "    if USE_CUDA:\n",
    "        all_decoder_outputs = all_decoder_outputs.cuda()\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        decoder_context = decoder_context.cuda()\n",
    "    \n",
    "    if train:\n",
    "        use_teacher_forcing = np.random.random() < tf_ratio\n",
    "    else:\n",
    "        use_teacher_forcing = False\n",
    "    \n",
    "    if use_teacher_forcing:        \n",
    "        # Use targets as inputs\n",
    "        for di in range(target_batches.shape[0]):\n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input.unsqueeze(0), decoder_context, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            all_decoder_outputs[di] = decoder_output\n",
    "            decoder_input = target_batches[di]\n",
    "    else:        \n",
    "        # Use decoder output as inputs\n",
    "        for di in range(target_batches.shape[0]):            \n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input.unsqueeze(0), decoder_context, decoder_hidden, encoder_outputs) \n",
    "            \n",
    "            all_decoder_outputs[di] = decoder_output\n",
    "            \n",
    "            # Greedy approach, take the word with highest probability\n",
    "            topv, topi = decoder_output.data.topk(1)            \n",
    "            decoder_input = Variable(torch.LongTensor(topi.cpu()).squeeze())\n",
    "            if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "        \n",
    "    del decoder_output\n",
    "    del decoder_hidden\n",
    "        \n",
    "    return all_decoder_outputs, target_batches\n",
    "\n",
    "def train_luong(input_batches, target_batches, batch_size, train=True, adj_arc_in=None, adj_arc_out=None, adj_lab_in=None, adj_lab_out=None, mask_in=None, mask_out=None, mask_loop=None):\n",
    "    \n",
    "    # Zero gradients of both optimizers\n",
    "    if train:\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "    loss = 0 # Added onto for each word\n",
    "    all_decoder_outputs, target_batches = pass_batch_luong(batch_size, input_batches, target_batches, train, adj_arc_in, adj_arc_out, adj_lab_in, adj_lab_out, mask_in, mask_out, mask_loop)\n",
    "    \n",
    "    # Loss calculation and backpropagation\n",
    "    loss = criterion(all_decoder_outputs.view(-1, decoder.output_size), target_batches.contiguous().view(-1))\n",
    "    \n",
    "    if train:\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "        torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        \n",
    "        if gcn1:\n",
    "            torch.nn.utils.clip_grad_norm_(gcn1.parameters(), clip)\n",
    "            gcn1_optimizer.step()\n",
    "\n",
    "    del all_decoder_outputs\n",
    "    del target_batches\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configure models\n",
    "hidden_size_rnn = 512\n",
    "hidden_size_graph = 512\n",
    "emb_size=300\n",
    "n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 50\n",
    "\n",
    "# Configure training/optimization\n",
    "clip = 10.0\n",
    "learning_rate_graph = 0.0002\n",
    "n_epochs = 20\n",
    "print_every = 10\n",
    "validate_loss_every = 50\n",
    "validate_acc_every = 2 * validate_loss_every\n",
    "tf_ratio = 0.5\n",
    "best_bleu = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "encoder = Encoder(len(input_lang.vocab.itos), hidden_size_rnn, emb_size, n_layers=n_layers, dropout=dropout, USE_CUDA=USE_CUDA)\n",
    "decoder = Decoder_luong('general', hidden_size_graph, len(output_lang.vocab.itos), 300, n_layers=2 * n_layers, dropout=dropout, USE_CUDA=USE_CUDA)\n",
    "gcn1 = Sintactic_GCN(hidden_size_rnn, hidden_size_graph, num_labels=len(DEP_LABELS))\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.Adam(encoder.parameters())\n",
    "decoder_optimizer = optim.Adam(decoder.parameters())\n",
    "gcn1_optimizer = optim.Adam(gcn1.parameters())#, learning_rate_graph)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA:\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "    gcn1 = gcn1.cuda()\n",
    "    \n",
    "# Keep track of time elapsed and running averages\n",
    "start = time.time()\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "validation_bleu = []\n",
    "\n",
    "print_loss_total = 0 # Reset every print_every\n",
    "plot_loss_total = 0 # Reset every plot_every"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 4s (- 8m 56s) (10 0.83%) train_loss: 5.5433\n",
      "0m 6s (- 6m 40s) (20 1.67%) train_loss: 2.3962\n",
      "0m 8s (- 5m 50s) (30 2.50%) train_loss: 2.1459\n",
      "0m 11s (- 5m 21s) (40 3.33%) train_loss: 2.1469\n",
      "0m 13s (- 5m 7s) (50 4.16%) train_loss: 2.0643\n",
      "0m 15s (- 5m 1s) (60 5.00%) train_loss: 2.0653\n",
      "0m 18s (- 4m 57s) (70 5.83%) train_loss: 2.1074\n",
      "0m 20s (- 4m 52s) (80 6.66%) train_loss: 2.0922\n",
      "0m 23s (- 4m 48s) (90 7.49%) train_loss: 2.1566\n",
      "0m 25s (- 4m 45s) (100 8.33%) train_loss: 1.9920\n",
      "0m 28s (- 4m 42s) (110 9.16%) train_loss: 1.9252\n",
      "0m 30s (- 4m 39s) (120 9.99%) train_loss: 1.9822\n",
      "0m 33s (- 4m 35s) (130 10.82%) train_loss: 1.8853\n",
      "0m 35s (- 4m 32s) (140 11.66%) train_loss: 1.8869\n",
      "0m 39s (- 4m 37s) (150 12.49%) train_loss: 1.8213\n",
      "0m 43s (- 4m 41s) (160 13.32%) train_loss: 1.7827\n",
      "0m 46s (- 4m 44s) (170 14.15%) train_loss: 1.7572\n",
      "0m 50s (- 4m 47s) (180 14.99%) train_loss: 1.7951\n",
      "0m 54s (- 4m 49s) (190 15.82%) train_loss: 1.7455\n",
      "0m 58s (- 4m 50s) (200 16.65%) train_loss: 1.7772\n",
      "1m 1s (- 4m 51s) (210 17.49%) train_loss: 1.7657\n",
      "1m 5s (- 4m 52s) (220 18.32%) train_loss: 1.7416\n",
      "1m 9s (- 4m 51s) (230 19.15%) train_loss: 1.8612\n",
      "1m 12s (- 4m 50s) (240 19.98%) train_loss: 1.7539\n",
      "1m 15s (- 4m 48s) (250 20.82%) train_loss: 1.5999\n",
      "1m 19s (- 4m 47s) (260 21.65%) train_loss: 1.7584\n",
      "1m 23s (- 4m 46s) (270 22.48%) train_loss: 1.6611\n",
      "1m 26s (- 4m 45s) (280 23.31%) train_loss: 1.7680\n",
      "1m 30s (- 4m 44s) (290 24.15%) train_loss: 1.7306\n",
      "1m 34s (- 4m 43s) (300 24.98%) train_loss: 1.7626\n",
      "1m 38s (- 4m 41s) (310 25.81%) train_loss: 1.7287\n",
      "1m 41s (- 4m 40s) (320 26.64%) train_loss: 1.6571\n",
      "1m 45s (- 4m 38s) (330 27.48%) train_loss: 1.7704\n",
      "1m 48s (- 4m 35s) (340 28.31%) train_loss: 1.7828\n",
      "1m 52s (- 4m 33s) (350 29.14%) train_loss: 1.7588\n",
      "1m 56s (- 4m 31s) (360 29.98%) train_loss: 1.7041\n",
      "1m 59s (- 4m 28s) (370 30.81%) train_loss: 1.7498\n",
      "2m 3s (- 4m 26s) (380 31.64%) train_loss: 1.6711\n",
      "2m 6s (- 4m 23s) (390 32.47%) train_loss: 1.6425\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs): \n",
    "    # Shuffle data\n",
    "    id_aux = np.random.permutation(np.arange(len(pairs_train)))\n",
    "    pairs_train = pairs_train[id_aux]\n",
    "    arr_dep_train = arr_dep_train[id_aux]\n",
    "    \n",
    "    # Get the batches for this epoch\n",
    "    input_batches, target_batches = generate_batches(input_lang, output_lang, batch_size, pairs_train, return_dep_tree=True, arr_dep=arr_dep_train, max_degree=6, USE_CUDA=USE_CUDA)\n",
    "    print_loss_total = 0\n",
    "    for batch_ix, (input_batch, target_var) in enumerate(zip(input_batches, target_batches)):\n",
    "    \n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        gcn1.train()\n",
    "    \n",
    "        [input_var, adj_arc_in, adj_arc_out, adj_lab_in, adj_lab_out, mask_in, mask_out, mask_loop] = input_batch\n",
    "        # Run the train function\n",
    "        loss = train_luong(input_var, target_var, input_var.size(1), \n",
    "                    True, adj_arc_in, adj_arc_out, adj_lab_in, adj_lab_out, mask_in, mask_out, mask_loop)\n",
    "            \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Keep track of loss\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if batch_ix == 0: continue\n",
    "\n",
    "        if batch_ix % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "            train_losses.append(loss)\n",
    "\n",
    "            print(f'{time_since(start, batch_ix / len(input_batches))} ({batch_ix} {batch_ix / len(input_batches) * 100:.2f}%) train_loss: {print_loss_avg:.4f}')\n",
    "    \n",
    "    input_batches, target_batches = generate_batches(input_lang, output_lang, batch_size, pairs_valid, return_dep_tree=True, arr_dep=arr_dep_train, max_degree=6, USE_CUDA=USE_CUDA)\n",
    "    print_loss_total = 0\n",
    "    for input_batch, target_var in zip(input_batches, target_batches):\n",
    "    \n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        gcn1.eval()\n",
    "    \n",
    "        [input_var, adj_arc_in, adj_arc_out, adj_lab_in, adj_lab_out, mask_in, mask_out, mask_loop] = input_batch\n",
    "        # Run the train function\n",
    "        loss = train_luong(input_var, target_var, input_var.size(1), \n",
    "                     False, adj_arc_in, adj_arc_out, adj_lab_in, adj_lab_out, mask_in, mask_out, mask_loop)\n",
    "        \n",
    "        print_loss_total += loss\n",
    "    val_loss = print_loss_total / len(input_batches)\n",
    "    validation_losses.append(val_loss)\n",
    "    # Evaluating Bleu\n",
    "    evaluator = Evaluator(encoder, decoder, gcn1, None, input_lang, output_lang, MAX_LENGTH, True)\n",
    "    candidates, references = evaluator.get_candidates_and_references(pairs_test, arr_dep_test, k_beams=1)\n",
    "    bleu = BLEU(candidates, [references])\n",
    "    if bleu[0] > best_bleu:\n",
    "        best_bleu = bleu[0]\n",
    "        torch.save(encoder.state_dict(), 'encoder_graph.pkl')\n",
    "        torch.save(decoder.state_dict(), 'decoder_graph.pkl')\n",
    "        torch.save(gcn1.state_dict(), 'gcn_graph.pkl')\n",
    "    validation_bleu.append(bleu)\n",
    "    print(f'val_loss: {val_loss:.4f} - bleu: {bleu}', end=' ')\n",
    "\n",
    "    # Prevent overflow gpu memory\n",
    "    del evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
