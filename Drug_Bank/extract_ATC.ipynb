{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import numpy as np\n",
    "\n",
    "# target url\n",
    "url = 'https://go.drugbank.com/'\n",
    "# for ATC \n",
    "# url = 'https://go.drugbank.com/atc'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drug bank common name list\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('D:/WorkSpace/master_thesis/Drug_Bank/drugbank_vocabulary.csv')\n",
    "df['ATC'] = 0\n",
    "drugName = df['Common name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# webdriver setting\n",
    "# driver.quit()\n",
    "# Chromedriver\n",
    "driver = webdriver.Chrome(executable_path=r'D:\\WorkSpace\\master_thesis\\chromedriver_win32\\chromedriver.exe')\n",
    "driver.get(url)\n",
    "\n",
    "\n",
    "# Phantomjs\n",
    "# driver = webdriver.PhantomJS(executable_path=r'D:\\WorkSpace\\phantomjs-2.1.1-windows\\bin\\phantomjs.exe')\n",
    "# driver.get(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AppURLopener' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-e3900970ff34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFancyURLopener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAppURLopener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'AppURLopener' is not defined"
     ]
    }
   ],
   "source": [
    "# from urllib.request import FancyURLopener\n",
    "     \n",
    "# temp = AppURLopener().open(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-27790ac0b981>:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['ATC'][i] = np.NaN\n",
      "C:\\Users\\paten\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re Connection\n",
      "Re Connection\n",
      "Re Connection\n",
      "Re Connection\n",
      "Re Connection\n"
     ]
    }
   ],
   "source": [
    "# ATC Classificatio page\n",
    "from urllib.request import FancyURLopener\n",
    "\n",
    "for i in range(1564, len(drugName)):\n",
    "# for i in range(3):\n",
    "    # search with drug name\n",
    "    try:\n",
    "        search_window  = driver.find_element_by_xpath('//*[@id=\"query\"]')\n",
    "        search_window.send_keys(drugName[i])\n",
    "        search_window.send_keys(Keys.ENTER)\n",
    "    except:\n",
    "        print(\"Re Connection\")\n",
    "        driver.quit()\n",
    "#         driver = webdriver.PhantomJS(executable_path=r'D:\\WorkSpace\\phantomjs-2.1.1-windows\\bin\\phantomjs.exe')\n",
    "        driver = webdriver.Chrome(executable_path=r'D:\\WorkSpace\\master_thesis\\chromedriver_win32\\chromedriver.exe')\n",
    "        driver.get(url)\n",
    "    \n",
    "    # Scraping ATC Code\n",
    "    try:\n",
    "        atcData = driver.find_element_by_css_selector(\"body > main > div > div > div.drug-card > div.card-content.px-md-4.px-sm-2.pb-md-4.pb-sm-2 > dl:nth-child(10) > dd:nth-child(2) > a\")\n",
    "        atcCode = atcData.text.split()[0]\n",
    "\n",
    "        df['ATC'][i] = atcCode\n",
    "    except: # ACT code가 없을 경우\n",
    "        df['ATC'][i] = np.NaN\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drug name으로 검색 \n",
    "search_window  = driver.find_element_by_xpath('//*[@id=\"query\"]')\n",
    "search_window.send_keys(drugName[1596])\n",
    "search_window.send_keys(Keys.ENTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{(4Z)-2-[(1R,2R)-1-Amino-2-hydroxypropyl]-4-[(4-amino-1H-indol-3-yl)methylene]-5-oxo-4,5-dihydro-1H-imidazol-1-yl}acetic acid'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drugName[1566]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "atc_code = soup.select_one('body > main > div > div > div.drug-card > div.card-content.px-md-4.px-sm-2.pb-md-4.pb-sm-2')\n",
    "\n",
    "body > main > div > div > div.drug-card > div.card-content.px-md-4.px-sm-2.pb-md-4.pb-sm-2 > dl:nth-child(10) > dd:nth-child(2) > a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(atc_code)\n",
    "# atcCode = atc_code.text\n",
    "# print(type(atcCode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_Data(html, rank):\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    title = soup.select_one('#yDetailTopWrap > div.topColRgt > div.gd_infoTop > div > h2')\n",
    "    author = soup.select_one('#yDetailTopWrap > div.topColRgt > div.gd_infoTop > span.gd_pubArea > span.gd_auth')\n",
    "    try:\n",
    "        author.span.extract()\n",
    "        author = author.text.strip()\n",
    "    except:\n",
    "        author = author.text.split('글/')[0].strip()\n",
    "        if author.find('원저/') != -1:\n",
    "            author = author.split('원저/')[1].strip()\n",
    "        else:\n",
    "            author = author.split('저')[0].strip()\n",
    "    \n",
    "    publisher = soup.select_one('#yDetailTopWrap > div.topColRgt > div.gd_infoTop > span.gd_pubArea > span.gd_pub > a')\n",
    "    publication = soup.select_one('#yDetailTopWrap > div.topColRgt > div.gd_infoTop > span.gd_pubArea > span.gd_date')\n",
    "    price = soup.select_one('#yDetailTopWrap > div.topColRgt > div.gd_infoBot > div.gd_infoTbArea > div.gd_infoTb').find('tr', {'class':'accentRow'}).find('em', {'class':'yes_m'}).text\n",
    "    \n",
    "    try:\n",
    "        review = soup.select_one('#infoset_reviewContentList').find('div', {'class':'review_cont'}).text\n",
    "        if review.find('.') != -1:\n",
    "            posPeriod  = review.find('.')\n",
    "        else:\n",
    "            posPeriod = 10000000000\n",
    "\n",
    "        if review.find('?') != -1:\n",
    "            posQuestion  = review.find('?')\n",
    "        else:\n",
    "            posQuestion = 10000000000\n",
    "\n",
    "        if posPeriod < posQuestion:\n",
    "            punctuation = '.'\n",
    "        elif posPeriod > posQuestion:\n",
    "            punctuation ='?'\n",
    "        else:\n",
    "            punctuation =''\n",
    "\n",
    "        review = re.split('(?<=[\\D])[\\.|\\n|\\?]', review)[1].strip()\n",
    "        review = review + punctuation        \n",
    "    except:\n",
    "        review = 'N/A'  \n",
    "\n",
    "    infoText = str(rank+1) + '. 제목: ' + title.text + ', 저자: ' + author + ', 출판사: ' + publisher.text.replace('\\n', '') + ', 판매가: ' + price + '원, 출간일: ' + publication.text\n",
    "    \n",
    "    f.write(infoText+'\\n')\n",
    "    f.write(review+'\\n')\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestSeller=driver.find_element_by_xpath('//*[@id=\"yesFixCorner\"]/dl/dd/ul[1]/li[1]/a')\n",
    "bestSeller.click()\n",
    "\n",
    "# Get Rank 1~10 Xpath\n",
    "rankXpath = [0]*10\n",
    "for i in range(10):\n",
    "    rankXpath[i] = '//*[@id=\"bestList\"]/ol/li[' + str(i+1) + ']/p[1]/a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-82ca651edd62>\u001b[0m in \u001b[0;36mparsing_Data\u001b[1;34m(html, rank)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mauthor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mauthor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mauthor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'span'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d57a202b7980>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimplicitly_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mparsing_Data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-82ca651edd62>\u001b[0m in \u001b[0;36mparsing_Data\u001b[1;34m(html, rank)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mauthor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mauthor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mauthor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mauthor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'글/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mauthor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'원저/'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mauthor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mauthor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'원저/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "f = open('yes24_bestSeller_Top10.txt', 'w', encoding = 'utf8')\n",
    "    \n",
    "for i in range(len(rankXpath)):\n",
    "    driver.implicitly_wait(1)\n",
    "    rankElement = driver.find_element_by_xpath(rankXpath[i])\n",
    "    #rankElement.click()\n",
    "    rankElement.send_keys(Keys.ENTER)\n",
    "    driver.implicitly_wait(1)\n",
    "    html = driver.page_source\n",
    "    driver.implicitly_wait(1)\n",
    "    parsing_Data(html, i)\n",
    "    driver.back()\n",
    "    \n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 제목: 마음챙김의 시, 저자: 류시화 편, 출판사: 수오서재, 판매가: 11,700원, 출간일: 2020년 09월 17일\n",
      "시를 읽는 것은 자기 자신으로 돌아오는 것이고 세상을 경이롭게 여기는 것이며 여러 색의 감정을 경험하는 것이다.\n"
     ]
    }
   ],
   "source": [
    "#pagq별 테스트\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "#title = soup.select_one('#yDetailTopWrap > div.topColRgt > div.gd_infoTop > div > h2')\n",
    "title = soup.find('h2', {'class':'gd_name'})    \n",
    "author = soup.select_one('#yDetailTopWrap > div.topColRgt > div.gd_infoTop > span.gd_pubArea > span.gd_auth')\n",
    "\n",
    "try:\n",
    "    author.span.extract()\n",
    "    author = author.text.strip()\n",
    "except:\n",
    "    author = author.text.split('글/')[0].strip()\n",
    "    if author.find('원저/') != -1:\n",
    "        author = author.split('원저/')[1].strip()\n",
    "    else:\n",
    "        author = author.split('저')[0].strip()\n",
    "'''\n",
    "try:\n",
    "    author.span.extract()\n",
    "    author = author.text.strip()\n",
    "except:\n",
    "    author = author.text.split('글/')[0].strip()\n",
    "    if author.find('원저/') != -1:\n",
    "        author = author.split('원저/')[1].strip()\n",
    "    else:\n",
    "        author = author.split('저')[0].strip()\n",
    "'''\n",
    "\n",
    "publisher = soup.select_one('#yDetailTopWrap > div.topColRgt > div.gd_infoTop > span.gd_pubArea > span.gd_pub > a')\n",
    "publication = soup.select_one('#yDetailTopWrap > div.topColRgt > div.gd_infoTop > span.gd_pubArea > span.gd_date')\n",
    "price = soup.select_one('#yDetailTopWrap > div.topColRgt > div.gd_infoBot > div.gd_infoTbArea > div.gd_infoTb').find('tr', {'class':'accentRow'}).find('em', {'class':'yes_m'}).text\n",
    "\n",
    "try:\n",
    "    review = soup.select_one('#infoset_reviewContentList').find('div', {'class':'review_cont'}).text\n",
    "    if review.find('.') != -1:\n",
    "        posPeriod  = review.find('.')\n",
    "    else:\n",
    "        posPeriod = 10000000000\n",
    "\n",
    "    if review.find('?') != -1:\n",
    "        posQuestion  = review.find('?')\n",
    "    else:\n",
    "        posQuestion = 10000000000\n",
    "\n",
    "    if posPeriod < posQuestion:\n",
    "        punctuation = '.'\n",
    "    elif posPeriod > posQuestion:\n",
    "        punctuation ='?'\n",
    "    else:\n",
    "        punctuation =''\n",
    "\n",
    "    review = re.split('(?<=[\\D])[\\.|\\n|\\?]', review)[1].strip()\n",
    "    if len(review) > 50:\n",
    "        review = re.split(r'다 ', review)[0] + '다.'\n",
    "    else:\n",
    "        review = review + punctuation        \n",
    "except:\n",
    "    review = 'N/A'  \n",
    "\n",
    "infoText = '1. 제목: ' + title.text + ', 저자: ' + author + ', 출판사: ' + publisher.text.replace('\\n', '') + ', 판매가: ' + price + '원, 출간일: ' + publication.text\n",
    "\n",
    "print(infoText)\n",
    "print(review)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
