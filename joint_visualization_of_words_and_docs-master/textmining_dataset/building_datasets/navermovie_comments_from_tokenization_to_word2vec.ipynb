{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import navermovie_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Score Tokenizer (Cohesion + NounExtractor + Actor name dictionary)\n",
    "\n",
    "비지도학습 기반 단어 추출방법을 이용하는 토크나이저를 학습합니다.\n",
    "\n",
    "토크나이저의 성능은 명사 인식 능력에 큰 영향을 받습니다. 한국어에서 가장 많이 이용되는 품사이며, 미등록단어가 자주 등장하는 단어입니다. 특히나 word embedding 을 적용할 때에는 하나의 명사가 두 개 이상의 subwords 로 나뉘어지지 않는 것이 중요합니다.\n",
    "\n",
    "그렇기 때문에 soynlp 의 WordExtractor 로부터 학습되는 cohesion score 와 NounExtractor 로부터 학습되는 noun score 를 조합하여 이용합니다. 여기에, 이 도메인에서 알려진 단어사전인 영화배우 이름을 추가하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 6.847 Gb use memory 8.188 Gb\n",
      "all cohesion probabilities was computed. # words = 801752\n"
     ]
    }
   ],
   "source": [
    "from soynlp.word import WordExtractor\n",
    "from soynlp.noun import LRNounExtractor_v2\n",
    "from navermovie_comments import load_movie_comments\n",
    "\n",
    "idxs, texts, rates = load_movie_comments(large=True, tokenize=None)\n",
    "\n",
    "word_extractor = WordExtractor()\n",
    "word_extractor.train(texts)\n",
    "# {str:(L-score, R-score)}\n",
    "cohesions = word_extractor.all_cohesion_scores()\n",
    "# {str:L-score}\n",
    "cohesions = {sub:lscore for sub, (lscore, rscore) in cohesions.items()}\n",
    "\n",
    "\n",
    "# noun_extractor = LRNounExtractor_v2()\n",
    "# # {str:NounScore}\n",
    "# nouns = noun_extractor.train_extract(texts, min_noun_frequency=5)\n",
    "# # {str:noun-score}\n",
    "# nouns = {sub:score.score for sub, score in nouns.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../lovit_textmining_dataset/navermovie_comments/data/casting.pkl', 'rb') as f:\n",
    "    castings = pickle.load(f)\n",
    "\n",
    "actor_names = {credit['k_name'].replace(' ', '') for credits in castings.values() for credit in credits}\n",
    "actor_names = {name:1.0 for name in actor_names} \n",
    "# actor name as noun\n",
    "nouns.update(actor_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word score = noun score + 0.3 * cohesion\n",
    "word_scores = {sub: score + 0.3 * cohesions.get(sub, 0) for sub, score in nouns.items()}\n",
    "word_scores.update({sub: 0.3 * score for sub, score in cohesions.items() if not (sub in nouns)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습된 토크나이저를 테스트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영화리뷰들로학습한모델입니다\n",
      "['영화', '리뷰', '들로', '학습', '한', '모델', '입니다']\n",
      "\n",
      "인셉션에는디카프리오가출연했습니다풀네임은레오나르도디카프리오입니다\n",
      "['인셉션', '에는', '디카프리오', '가', '출연', '했', '습니다', '풀', '네임', '은', '레오나르도디카프리오', '입니다']\n",
      "\n",
      "이동진은영화평론가입니다\n",
      "['이동진은', '영화', '평론가', '입니다']\n",
      "\n",
      "1점2점일점이점별점을줍니다\n",
      "['1점', '2점', '일', '점이', '점', '별점', '을', '줍니다']\n",
      "\n",
      "영화관의종류에는imax3d4d포디4D등이있습니다\n",
      "['영화', '관의', '종류', '에는', 'imax', '3d', '4d', '포디', '4D', '등이', '있', '습니다']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from soynlp.tokenizer import MaxScoreTokenizer\n",
    "\n",
    "# combine noun score & cohesion\n",
    "# tokenizer = MaxScoreTokenizer(scores = word_scores)\n",
    "\n",
    "# only cohesion\n",
    "tokenizer = MaxScoreTokenizer(scores = cohesions)\n",
    "\n",
    "# test\n",
    "tests = [\n",
    "    '영화리뷰들로학습한모델입니다',\n",
    "    '인셉션에는디카프리오가출연했습니다풀네임은레오나르도디카프리오입니다',\n",
    "    '이동진은영화평론가입니다',\n",
    "    '1점2점일점이점별점을줍니다',\n",
    "    '영화관의종류에는imax3d4d포디4D등이있습니다'\n",
    "]\n",
    "for sent in tests:\n",
    "    print(sent)\n",
    "    print(tokenizer.tokenize(sent, ), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done large                    \n",
      "done small                    \n"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "\n",
    "dirname = '../lovit_textmining_dataset/navermovie_comments/data/'\n",
    "suffix = 'soynlp_cohesion'\n",
    "\n",
    "if False:\n",
    "    for large in [True, False]:\n",
    "        idxs, texts, rates = load_movie_comments(large=large, tokenize=None)\n",
    "        size = 'large'  if large else 'small'\n",
    "        path = '{}/data_{}_{}.txt'.format(dirname, size, suffix)\n",
    "        with open(path, 'w', encoding='utf-8') as f:\n",
    "            for i, (idx, text, rate) in enumerate(zip(idxs, texts, rates)):\n",
    "                if i % 10000 == 0:\n",
    "                    print('\\rtokenizing {} sents'.format(i), end='')\n",
    "                text_ = ' '.join(tokenizer.tokenize(text))\n",
    "                f.write('{}\\t{}\\t{}\\n'.format(idx, text_, rate))\n",
    "        print('\\rdone {}{}'.format(size, ' '*20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec, Doc2Vec training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from navermovie_comments import get_movie_comments_path\n",
    "\n",
    "path = get_movie_comments_path(large=True, tokenize='soynlp_unsup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "class Word2VecComments:\n",
    "    def __init__(self, path, verbose=False):\n",
    "        self.path = path\n",
    "        self.verbose = verbose\n",
    "        self.n_iter = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        # <idx, texts, rates>\n",
    "        with open(self.path, encoding='utf-8') as f:\n",
    "            for i, doc in enumerate(f):\n",
    "                if self.verbose and (i % 10000 == 0):\n",
    "                    print('\\riter={}, sents={} ...'.format(self.n_iter, i), end='')\n",
    "                yield self._tokenize(doc)\n",
    "            if self.verbose:\n",
    "                print('\\riter={}, sents={} done'.format(self.n_iter, i))\n",
    "            self.n_iter += 1\n",
    "\n",
    "    def _tokenize(self, doc):\n",
    "        idx, text, rate = doc.strip().split('\\t')\n",
    "        return text.split()\n",
    "\n",
    "class Doc2VecComments(Word2VecComments):\n",
    "    def _tokenize(self, doc):\n",
    "        idx, text, rate = doc.strip().split('\\t')\n",
    "        return TaggedDocument(\n",
    "                    words=text.split(), tags=['#%s' % idx]\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train with large size dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_corpus = Word2VecComments(path)\n",
    "doc2vec_corpus = Doc2VecComments(path)\n",
    "\n",
    "word2vec_model = Word2Vec(word2vec_corpus)\n",
    "doc2vec_model = Doc2Vec(doc2vec_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "dirname = '../lovit_textmining_dataset/navermovie_comments/models/'\n",
    "\n",
    "path = '{}/word2vec_large_soynlp_unsup_gensim3-6.pkl'.format(dirname)\n",
    "with open(path, 'wb') as f:\n",
    "    pickle.dump(word2vec_model, f)\n",
    "\n",
    "path = '{}/doc2vec_large_soynlp_unsup_gensim3-6.pkl'.format(dirname)\n",
    "with open(path, 'wb') as f:\n",
    "    pickle.dump(doc2vec_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train with small size dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=0, sents=294492 done\n",
      "iter=1, sents=294492 done\n",
      "iter=2, sents=294492 done\n",
      "iter=3, sents=294492 done\n",
      "iter=4, sents=294492 done\n",
      "iter=5, sents=294492 done\n",
      "CPU times: user 59.5 s, sys: 264 ms, total: 59.8 s\n",
      "Wall time: 29.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "path = get_movie_comments_path(large=False, tokenize='soynlp_unsup')\n",
    "word2vec_corpus = Word2VecComments(path, verbose=True)\n",
    "word2vec_model = Word2Vec(word2vec_corpus)\n",
    "\n",
    "path = '{}/word2vec_small_soynlp_unsup_gensim3-6.pkl'.format(dirname)\n",
    "with open(path, 'wb') as f:\n",
    "    pickle.dump(word2vec_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=0, sents=294492 done\n",
      "iter=1, sents=294492 done\n",
      "iter=2, sents=294492 done\n",
      "iter=3, sents=294492 done\n",
      "iter=4, sents=294492 done\n",
      "iter=5, sents=294492 done\n",
      "CPU times: user 1min 5s, sys: 396 ms, total: 1min 6s\n",
      "Wall time: 32.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "path = get_movie_comments_path(large=False, tokenize='komoran')\n",
    "word2vec_corpus = Word2VecComments(path, verbose=True)\n",
    "word2vec_model = Word2Vec(word2vec_corpus)\n",
    "\n",
    "path = '{}/word2vec_small_komoran_gensim3-6.pkl'.format(dirname)\n",
    "with open(path, 'wb') as f:\n",
    "    pickle.dump(word2vec_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
